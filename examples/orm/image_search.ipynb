{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d06351-5897-419e-a19b-914c16a32724",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pymilvus torch gdown torchvision tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c61003-0876-4a56-8055-3aeee8ce005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import zipfile\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=1OYDHLEy992qu5C4C8HV5uDIkOWRTAR1_'\n",
    "output = './paintings.zip'\n",
    "gdown.download(url, output)\n",
    "\n",
    "with zipfile.ZipFile(\"./paintings.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./paintings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e25577-67ee-4115-8664-40e761998414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Milvus Setup Arguments\n",
    "COLLECTION_NAME = 'image_search'\n",
    "DIMENSION = 2048\n",
    "\n",
    "# Inference Arguments\n",
    "BATCH_SIZE = 128\n",
    "TOP_K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df37ecb-088f-46c3-9fc8-a88cf222417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient, DataType\n",
    "\n",
    "client = MilvusClient(\"./milvus_demo.db\")\n",
    "\n",
    "# Remove any previous collections with the same name\n",
    "if client.has_collection(COLLECTION_NAME):\n",
    "    client.drop_collection(COLLECTION_NAME)\n",
    "\n",
    "# Create schema\n",
    "schema = client.create_schema(\n",
    "    auto_id=False,\n",
    "    enable_dynamic_field=True,\n",
    ")\n",
    "\n",
    "# Add fields to schema\n",
    "schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True, auto_id=True)\n",
    "schema.add_field(field_name=\"filepath\", datatype=DataType.VARCHAR, max_length=256)\n",
    "schema.add_field(field_name=\"image_embedding\", datatype=DataType.FLOAT_VECTOR, dim=DIMENSION)\n",
    "\n",
    "# Prepare index params\n",
    "index_params = client.prepare_index_params()\n",
    "index_params.add_index(\n",
    "    field_name=\"image_embedding\",\n",
    "    index_type=\"FLAT\",\n",
    "    index_name=\"vector_index\",\n",
    "    metric_type=\"COSINE\"\n",
    ")\n",
    "\n",
    "# Create a collection with the index loaded simultaneously\n",
    "client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    schema=schema,\n",
    "    index_params=index_params\n",
    ")\n",
    "\n",
    "res = client.get_load_state(\n",
    "    collection_name=COLLECTION_NAME\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256927fb-245c-4466-afe4-3d08d55d2a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load the embedding model with the last layer removed\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "model.eval()\n",
    "\n",
    "# Preprocessing for images\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9292c195-d2f3-447c-9a4e-382274ecb700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Embed function that embeds the batch and inserts it\n",
    "def embed(data_array):\n",
    "    with torch.no_grad():\n",
    "        output = model(torch.stack(data_array[0])).squeeze().tolist()\n",
    "        data = [\n",
    "            {\"filepath\": data_array[1][i], \"image_embedding\": output[i]} for i in range(len(data_array[0]))\n",
    "        ]\n",
    "        client.insert(collection_name=COLLECTION_NAME, data=data)\n",
    "\n",
    "data_batch = [[], []]\n",
    "\n",
    "# Get the filepaths of the images\n",
    "paths = glob.glob('./paintings/paintings/**/*.jpg', recursive=True)\n",
    "len(paths)\n",
    "\n",
    "# Read the images into batches for embedding and insertion\n",
    "for path in tqdm(paths):\n",
    "    im = Image.open(path).convert('RGB')\n",
    "    data_batch[0].append(preprocess(im))\n",
    "    data_batch[1].append(path)\n",
    "    if len(data_batch[0]) % BATCH_SIZE == 0:\n",
    "        embed(data_batch)\n",
    "        data_batch = [[], []]\n",
    "\n",
    "# Embed and insert the remainder\n",
    "if len(data_batch[0]) != 0:\n",
    "    embed(data_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bc2818-b0bf-4a66-8235-fe32fb056ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the search images\n",
    "def embed(data):\n",
    "    with torch.no_grad():\n",
    "        ret = model(torch.stack(data))\n",
    "        # If more than one image, use squeeze\n",
    "        if len(ret) > 1:\n",
    "            return ret.squeeze().tolist()\n",
    "        # Squeeze would remove batch for single image, so using flatten\n",
    "        else:\n",
    "            return torch.flatten(ret, start_dim=1).tolist()\n",
    "\n",
    "data_batch = [[], []]\n",
    "\n",
    "# Get the filepaths of the search images\n",
    "search_paths = glob.glob('./paintings/test_paintings/**/*.jpg', recursive=True)\n",
    "len(search_paths)\n",
    "\n",
    "for path in search_paths:\n",
    "    im = Image.open(path).convert('RGB')\n",
    "    data_batch[0].append(preprocess(im))\n",
    "    data_batch[1].append(path)\n",
    "\n",
    "embeds = embed(data_batch[0])\n",
    "res = client.search(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    data=embeds,\n",
    "    anns_field='image_embedding',\n",
    "    limit=TOP_K,\n",
    "    output_fields=['filepath'])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f812b8f-7bfb-4779-9ceb-b34c97831f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Show the image results\n",
    "f, axarr = plt.subplots(len(data_batch[1]), TOP_K + 1, figsize=(20, 10), squeeze=False)\n",
    "\n",
    "for hits_i, hits in enumerate(res):\n",
    "    axarr[hits_i][0].imshow(Image.open(data_batch[1][hits_i]))\n",
    "    axarr[hits_i][0].set_axis_off()\n",
    "    axarr[hits_i][0].set_title('Search Time: ' + str(finish - start))\n",
    "    for hit_i, hit in enumerate(hits):\n",
    "        axarr[hits_i][hit_i + 1].imshow(Image.open(hit['entity']['filepath']))\n",
    "        axarr[hits_i][hit_i + 1].set_axis_off()\n",
    "        axarr[hits_i][hit_i + 1].set_title('Distance: ' + str(hit['distance']))\n",
    "\n",
    "# Save the search result in a separate image file alongside your script.\n",
    "plt.savefig('search_result.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e238bbfa-8300-4f55-b663-081f5392ab78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
